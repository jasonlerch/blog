[
  {
    "objectID": "posts/copy-select-files-pydpiper/index.html",
    "href": "posts/copy-select-files-pydpiper/index.html",
    "title": "Copying important pydpiper outputs",
    "section": "",
    "text": "A common scenario is to run a pydpiper pipeline, such as MBM.py, on a cluster, but to want to run the statistical analyses on a different computer. These pipelines can have a pretty hefty disk usage footprint, so it’s not always wise to copy the entire output, but to only get the needed bits. These needed bits are typically:\n\nthe final nonlinear average produced from the data - this will be used to visualize stats maps on\nthe atlas labels for the final nonlinear average, also for visualization purposes\nthe blurred Jacobian determinants (usually both absolute and relative)\nthe atlas labels for each image in the pipeline\n\nrsync to the rescue. Below are the two rsync commands I use to get just those files described above. In this example, I ran the pipeline on ComputeCanada’s graham cluster, in a folder called myproject on /scratch/jlerch, with the pipeline name as mypipe-2023-10-10.\n\n# first the final non-linear average and it's corresponding atlas labels\nrsync -uvrm --include=\"*/\" --include=\"*nlin-3.mnc\" --include=\"*voted*\" --exclude=\"*\" jlerch@graham.computecanada.ca:/scratch\n/jlerch/myproject/mypipe-2023-10-19_nlin/ .\n\n# next all the processed files, keeping the 0.2mm blurred jacobians\n# and the atlas labels per file\nrsync -uvr --include=\"*/\" --include=\"*voted*\" --include=\"*fwhm0.2.mnc\" --exclude=\"*\" jlerch@graham.computecanada.ca:/scratch\n/jlerch/myproject/mypipe-2023-10-19_processed/ .\n\nThe resulting output will keep the directory structure of the pipeline, but only copy the desired few files. Obviously it is easy to modify those commands to also keep, for example, the different resampled files. Lots more help out there on how to tune rsync to your purposes"
  },
  {
    "objectID": "posts/better-MRIcrotome-outlines/index.html",
    "href": "posts/better-MRIcrotome-outlines/index.html",
    "title": "Better MRIcrotome brain outlines",
    "section": "",
    "text": "Brain outlines are used routinely in MRIcrotome sliceSeries for showing slice locations, plus they can have value in their own right for replacing the full grey-scale background with an outline. By default the outlines take a bit of trial and error to find the right isointensity lines; in some cases no truly good outlines can be generated. Here I will show an alternate way of generating an outline based on having a hierarchical segmentation present."
  },
  {
    "objectID": "posts/better-MRIcrotome-outlines/index.html#setting-up",
    "href": "posts/better-MRIcrotome-outlines/index.html#setting-up",
    "title": "Better MRIcrotome brain outlines",
    "section": "1 Setting up",
    "text": "1 Setting up\nFirst we’ll load an existing dataset and generate the hierarchical segmentation representations. For those at Oxford using the BMRC cluster you can follow along, for anyone else you’ll have to substitute your own dataset.\nFirst step, read the information about the scans.\n\nlibrary(tidyverse)\ngfw2 &lt;- read_csv(\"exercise-gfw2.csv\")\n\nSo at this point we have the gf variable containing info about all our files, and gfw2 subsetting them just to the two week timepoint (this is from mice being given access to an exercise wheel).\nNext we’ll load the final non-linear average, the segmented final nonlinear average, and build all the volumes.\n\nlibrary(RMINC)\n\n# read the anatomy and labels \nnlin &lt;- mincArray(mincGetVolume(\"/well/lerch/users/yrf023/plasticity/plasticity-2023-03-10_nlin/plasticity-2023-03-10-nlin-3.mnc\"))\nlabels &lt;- mincArray(mincGetVolume(\"/well/lerch/users/yrf023/plasticity/plasticity-2023-03-10_nlin/plasticity-2023-03-10-nlin-3/plasticity-2023-03-10-nlin-3_voted.mnc\"))\n\n# get all the volumes\nallvolsw2 &lt;- anatGetAll(gfw2$labels, defs=\"/well/lerch/shared/tools/atlases/Dorr_2008_Steadman_2013_Ullmann_2013/mappings/Dorr_2008_Steadman_2013_Ullmann_2013_mapping_of_labels.csv\", method=\"labels\")\n\nAnd now we put the labelled atlas into it’s hierarchy.\n\n# there's a warning that spews everywhere that needs to be fixed in RMINC, but causes no harm.\n# so for now I'll suppress these warnings\nsuppressWarnings({\nhdefs &lt;- makeMICeDefsHierachical(\"/well/lerch/shared/tools/atlases/Dorr_2008_Steadman_2013_Ullmann_2013/mappings/Dorr_2008_Steadman_2013_Ullmann_2013_mapping_of_labels.csv\", \"/well/lerch/shared/tools/atlases/Allen_Brain/Allen_hierarchy_definitions.json\")\nhvolsw2 &lt;- addVolumesToHierarchy(hdefs, allvolsw2)\n})"
  },
  {
    "objectID": "posts/better-MRIcrotome-outlines/index.html#a-basic-example",
    "href": "posts/better-MRIcrotome-outlines/index.html#a-basic-example",
    "title": "Better MRIcrotome brain outlines",
    "section": "2 A basic example",
    "text": "2 A basic example\nLet’s do a basic example of running a simple linear model, showing it on a brain alongside a slice indicator and legend.\n\nlibrary(MRIcrotome)\n\n\nAttaching package: 'MRIcrotome'\n\n\nThe following object is masked from 'package:graphics':\n\n    legend\n\nlibrary(grid)\n\n# a simple linear model against group\nvs &lt;- mincLm(reljacs ~ group, gfw2, mask = \"/well/lerch/users/yrf023/plasticity/plasticity-2023-03-10_nlin/plasticity-2023-03-10-nlin-3_mask.mnc\")\n\nMethod: lm\nNumber of volumes: 18\nVolume sizes: 161 319 210\nN: 18 P: 2\nIn slice \n 0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \nDone\n\n# show it as a 3 by 3 slice series\nsliceSeries(nrow=3, ncol=3, begin=60, end=250) %&gt;%\n  anatomy(nlin, low=700, high=1400) %&gt;%\n  overlay(mincArray(vs, \"tvalue-grouprunning\"), low=2, high=6, symmetric=T) %&gt;%\n  legend(\"t-statistics\") %&gt;%\n  contourSliceIndicator(nlin, levels=c(700, 1400)) %&gt;%\n  draw()\n\n\n\n\nSo that looks roughly right - the slice indicator, at the top right, gives a pretty good idea of where the brain is located. But it’s not great; let’s try a few different contours to see what might look better.\n\nsliceSeries(nrow=3, ncol=3, begin=60, end=250) %&gt;%\n  anatomy(nlin, low=700, high=1400) %&gt;%\n  contours(nlin, levels=c(700, 900, 1100, 1400), col = c(\"red\", \"green\", \"blue\", \"purple\")) %&gt;%\n  draw()\n\n\n\n\nLots of fiddling could be done getting the exact right intensity contours, but it’ll be a challenge to get it just right."
  },
  {
    "objectID": "posts/better-MRIcrotome-outlines/index.html#contours-based-on-segmentations",
    "href": "posts/better-MRIcrotome-outlines/index.html#contours-based-on-segmentations",
    "title": "Better MRIcrotome brain outlines",
    "section": "3 Contours based on segmentations",
    "text": "3 Contours based on segmentations\nSo here’s the trick - the perfect outline is inherently in our segmentation. First, let’s view the segmentation\n\nsliceSeries(nrow=5, ncol=1, begin=60, end=250) %&gt;%\n  anatomy(nlin, low=700, high=1400) %&gt;%\n  addtitle(\"anatomy\") %&gt;%\n  sliceSeries() %&gt;%\n  anatomy() %&gt;%\n  overlay(hanatToVolume(hvolsw2, labels, \"color_hex_triplet\"), low=0, high=1) %&gt;%\n  addtitle(\"segmentation\") %&gt;%\n  draw()\n\n\n\n\nNow the key part here is that the labels are in a hierarchy. A clean outline could thus be taken if we use just the first parts of that hierarchy:\n\nlibrary(data.tree)\n# create a copy of the hierarchical tree\nhdefs2 &lt;- Clone(hvolsw2)\n# and prune it to just the first four levels \nhdefs2$Prune(function(x) x$level&lt;4)\n\nWarning: 'hdefs2$Prune' is deprecated.\nUse 'Prune(node, ...)' instead.\nSee help(\"Deprecated\")\n\n\n[1] 495\n\n# print it to see what it looks like\nprint(hdefs2, limit=Inf)\n\n                                               levelName\n1  root2                                                \n2   &lt;U+00A6&gt;--Basic cell groups and regions             \n3   &lt;U+00A6&gt;   &lt;U+00A6&gt;--Cerebrum                       \n4   &lt;U+00A6&gt;   &lt;U+00A6&gt;--Brain stem                     \n5   &lt;U+00A6&gt;   &lt;U+00B0&gt;--Cerebellum                     \n6   &lt;U+00A6&gt;--fiber tracts                              \n7   &lt;U+00A6&gt;   &lt;U+00A6&gt;--cranial nerves                 \n8   &lt;U+00A6&gt;   &lt;U+00A6&gt;--medial forebrain bundle system \n9   &lt;U+00A6&gt;   &lt;U+00A6&gt;--cerebellum related fiber tracts\n10  &lt;U+00A6&gt;   &lt;U+00A6&gt;--lateral forebrain bundle system\n11  &lt;U+00A6&gt;   &lt;U+00B0&gt;--extrapyramidal fiber systems   \n12  &lt;U+00B0&gt;--ventricular systems                       \n13      &lt;U+00A6&gt;--cerebral aqueduct                     \n14      &lt;U+00A6&gt;--fourth ventricle                      \n15      &lt;U+00A6&gt;--lateral ventricle                     \n16      &lt;U+00B0&gt;--third ventricle                       \n\n\nNow create an outline based on this pruned hierarchy\n\nsliceSeries(nrow=5, ncol=1, begin=60, end=250) %&gt;%\n  anatomy(nlin, low=700, high=1400) %&gt;%\n  sliceSeries() %&gt;%\n  anatomy() %&gt;%\n  overlay(hanatToVolume(hvolsw2, labels, \"color_hex_triplet\"), low=0, high=1) %&gt;%\n  sliceSeries() %&gt;% anatomy() %&gt;%\n  contours(hanatToVolume(hdefs2, labels, \"position\"), levels=c(0.5, 1.5, 2.5, 3.5), col=\"red\") %&gt;%\n  draw()\n\n\n\n\nAnd there you have an almost perfect outline of the brain. So let’s recreate that earlier figure with the cleaner outline:\n\nsliceSeries(nrow=3, ncol=3, begin=60, end=250) %&gt;%\n  anatomy(nlin, low=700, high=1400) %&gt;%\n  overlay(mincArray(vs, \"tvalue-grouprunning\"), low=2, high=6, symmetric=T) %&gt;%\n  legend(\"t-statistics\") %&gt;%\n  contourSliceIndicator(hanatToVolume(hdefs2, labels, \"position\"), levels=c(0.5, 1.5, 2.5, 3.5)) %&gt;%\n  draw()\n\n\n\n\nVoila."
  },
  {
    "objectID": "posts/minc-on-bmrc/index.html",
    "href": "posts/minc-on-bmrc/index.html",
    "title": "MINC tools on BMRC cluster",
    "section": "",
    "text": "Some bits and pieces on how to run the different MINC tools related to mouse imaging pipelines, prominently pydpiper, the viz tools, and RMINC, on Oxford’s BMRC cluster."
  },
  {
    "objectID": "posts/minc-on-bmrc/index.html#some-bmrc-basics",
    "href": "posts/minc-on-bmrc/index.html#some-bmrc-basics",
    "title": "MINC tools on BMRC cluster",
    "section": "Some BMRC basics",
    "text": "Some BMRC basics\nThe BMRC cluster login is described here. You obviously need an account first; how to request one is described here. Once you have an account an additional way to access BMRC is via the Open OnDemand interface, which is what I use for most tasks and will be important for visualization and RStudio.\nThree notes about what data is allowed on the BMRC cluster 1. Rodent MRI data is fine 2. Rodent video tracking data has to get approval first - so contact them before trying deeplabcut, for example. 3. Human MRI data, even if from an open dataset, has a somewhat onerous approval process in place as well before it is allowed."
  },
  {
    "objectID": "posts/minc-on-bmrc/index.html#using-minc-tools-via-the-singularity-container",
    "href": "posts/minc-on-bmrc/index.html#using-minc-tools-via-the-singularity-container",
    "title": "MINC tools on BMRC cluster",
    "section": "Using MINC tools via the singularity container",
    "text": "Using MINC tools via the singularity container\nOnce you have a shell open, there are two ways to access the MINC tools. For the majority of uses you’ll want to access them via our singularity container, located here: /well/lerch/shared/tools/mice.sif_latest.sif\nAll the main MINC tools are there, to be accessed like so:\n\nexport APPTAINER_BIND=/well/\napptainer exec /well/lerch/shared/tools/mice.sif_latest.sif mincinfo file.mnc\n\nI often set an alias to make life a bit easier:\n\nalias SE='apptainer exec /well/lerch/shared/tools/mice.sif_latest.sif'\n\nAnd then the commands can be accessed more simply\n\nSE mincinfo file.mnc"
  },
  {
    "objectID": "posts/minc-on-bmrc/index.html#using-rminc-on-bmrc",
    "href": "posts/minc-on-bmrc/index.html#using-rminc-on-bmrc",
    "title": "MINC tools on BMRC cluster",
    "section": "Using RMINC on BMRC",
    "text": "Using RMINC on BMRC\nYou can use RMINC via the container, though with the disadvantage of currently (this will be fixed once I find a bit of time) using the older R 3.6. More importantly, it’s more challenging to access the cluster queues from within the container. So R and RMINC can also be accessed via bare metal.\nTo launch RStudio, start an Open OnDemand session. Then, when prompted with this screen\n\nChoose RStudio. This will give some choices, like so:\n\n\n\nNotice two changes from the default here:\n\n\n\nSelect “WIN” from the partition; not strictly necessary, but there are usually available cores with lots of RAM here\nChange the “RAM in Gb”, the default is 4Gb, which won’t be enough. Go for 32 or even higher if you know you need lots of RAM.\n\nAt this point you’ll have an RStudio session in your web browser, running on a node on the BMRC cluster. Which is sweet.\nTo get RMINC working, you’ll need to do this once (i.e. only the first time you start an RStudio session).\n\nusethis::edit_r_profile()\n\nThis will open your R profile in the editor window. Enter these lines:\n\n.libPaths(c(\"/well/lerch/shared/tools/packages/RMINC/build/\", .libPaths()))\n\nThis will make sure that RMINC and MRIcrotome, as well as a few other packages that I’ve installed, are loadable."
  },
  {
    "objectID": "posts/minc-on-bmrc/index.html#running-pydpiper-on-bmrc",
    "href": "posts/minc-on-bmrc/index.html#running-pydpiper-on-bmrc",
    "title": "MINC tools on BMRC cluster",
    "section": "Running pydpiper on BMRC",
    "text": "Running pydpiper on BMRC\nThere are three steps to running a pydpiper pipeline on the BMRC cluster\n\nGenerate a makeflow file using one of the pydpiper pipelines, such as MBM.py\nFix a time allocation bug\nUse makeflow to run the pipeline\n\nHere’s an example of generating the makeflow file. See here for an annotation of what all the options are doing. Key here is setting the backend to be ‘makeflow’\n\nSE MBM.py --backend=makeflow --makeflow-opts='-h' --pipeline-name Yingshi-T2w-2023-08-11 --maget-registration-method minctracc --subject-matter mousebrain --init-model /well/lerch/shared/tools/initial-models/oxford-model-2023/oxford_t2w_mouse_60micron.mnc --run-maget --maget-atlas-library /well/lerch/shared/tools/atlases/Dorr_2008_Steadman_2013_Ullmann_2013_Richards_2011_Qiu_2016_Egan_2015_40micron/ex-vivo/ --maget-nlin-protocol /well/lerch/shared/tools/protocols/nonlinear/default_nlin_MAGeT_minctracc_prot.csv --maget-masking-nlin-protocol /well/lerch/shared/tools/protocols/nonlinear/default_nlin_MAGeT_minctracc_prot.csv --lsq12-protocol /well/lerch/shared/tools/protocols/linear/Pydpiper_testing_default_lsq12.csv --no-common-space-registration --lsq6-simple --num-executors 1 --files /well/lerch/users/yrf023/Yingshi-tests/native/*removed.mnc\n\nNext we fix the time variable. In short, pydpiper sets the default time for some registrations to be 48 hours. Unfortunately, this cannot be overwritten with command arguments at this point. (When I have some time I’ll fix that). But since the makeflow file is just a text file, we can just do a string substitution to change it to 24 hours\n\ncat Yingshi-T2w-2023-08-11_makeflow.jx | perl -npe 's/\"wall-time\"\\: 172800/\"wall-time\": 86400/' &gt; Yingshi-T2w-2023-08-11_makeflow_fixed.jx\n\nNow that we have a fixed up makeflow file, we can run it with makeflow itself. Right now makeflow is installed as a conda environment. Before the first run, edit your ~/.condarc file to contain the following bits:\n\nchannels:\n  - conda-forge\n  - bioconda\n  - defaults\n \npkgs_dirs:\n  - /well/lerch/shared/conda/${MODULE_CPU_TYPE}/pkgs\nenvs_dirs:\n  - /well/lerch/shared/conda/${MODULE_CPU_TYPE}/envs\n\nSee here for more details.\nAssuming that the ~/.condarc file is correct, you can now set your environment for running conda.\n\n# and run via makeflow\nmodule load Anaconda3/2022.05\neval \"$(conda shell.bash hook)\"\nconda activate cctools-env\n\nNow you can run makeflow itself:\n\n1makeflow -T slurm \\\n2-B '-p short,win' \\\n3--max-remote=500 \\\n4-o stderr.log \\\n5--shared-fs=/well,/gpfs3 \\\n6--singularity=/well/lerch/shared/tools/mice.sif_latest.sif \\\n7--singularity-opt='--bind=/well,/gpfs3' \\\n8--jx Yingshi-T2w-2023-08-11_makeflow_fixed.jx\n\n\n1\n\nWe tell makeflow to use the slurm backend, which is what BMRC uses.\n\n2\n\nWe tell it to use both the short and win queues\n\n3\n\nWe allow up to 500 jobs to be submitted at once.\n\n4\n\nWe capture some errors to stderr.log\n\n5\n\nWe need to tell it which filesystems are shared.\n\n6\n\nWe need to tell it run the commands itself inside the container.\n\n7\n\nWe need to tell the container which filesystems to bind.\n\n8\n\nAnd finally we pass it the fixed makeflow file.\n\n\n\n\nThat’s it - it will now run for a good while. I would thus recommend you run it inside a tmux session, so that you can log out and come back later to check on progress (or won’t lose progress if you get disconnected).\nA good way to check on the status of the pipeline is to look at the makeflowlog file that will be produced as the pipeline runs, or use ‘squeue –me’ to see which jobs are submitted to the cluster.\n(Also note, there should be a better way to run these pipelines via makeflow’s workqueue, but last I tried I couldn’t get it to run. Will return to that at some point.)"
  },
  {
    "objectID": "posts/MBM-command-illustrated/index.html",
    "href": "posts/MBM-command-illustrated/index.html",
    "title": "Annotated MBM command",
    "section": "",
    "text": "There is little good documentation of the different pydpiper pipelines, unfortunately. Here I will take a baby-step to remedying that in providing an annotated command for a pipeline I recently ran. This will hopefully provide at least some level of insight into all them thar options.\nHere’s the command, run on a set of test-data from Yingshi:\n1SE MBM.py \\\n2--backend=makeflow \\\n3--makeflow-opts='-h' \\\n4--pipeline-name Yingshi-T2w-2023-08-11 \\\n5--subject-matter mousebrain \\\n6--init-model /well/lerch/shared/tools/initial-models/oxford-model-2023/oxford_t2w_mouse_60micron.mnc \\\n7--lsq6-simple \\\n8--lsq12-protocol /well/lerch/shared/tools/protocols/linear/Pydpiper_testing_default_lsq12.csv \\\n9--run-maget \\\n10--maget-registration-method minctracc \\\n11--maget-atlas-library /well/lerch/shared/tools/atlases/Dorr_2008_Steadman_2013_Ullmann_2013_Richards_2011_Qiu_2016_Egan_2015_40micron/ex-vivo/ \\\n12--maget-nlin-protocol /well/lerch/shared/tools/protocols/nonlinear/default_nlin_MAGeT_minctracc_prot.csv \\\n--maget-masking-nlin-protocol /well/lerch/shared/tools/protocols/nonlinear/default_nlin_MAGeT_minctracc_prot.csv \\\n13--no-common-space-registration \\\n14--num-executors 1 \\\n15--files /well/lerch/users/yrf023/Yingshi-tests/native/*removed.mnc\n\n\n1\n\nThe command itself. The SE prefix here is an alias to running the command in a singularity container (see here)\n\n2\n\nThe backend. There are two options; the default (i.e. if you do not specify a backend), which is our own server-executor mode based on pyro (python remote objects). Here I am using makeflow, in which case pydpiper will output a makeflow file which can then be processed separately using the different makeflow tools.\n\n3\n\nThis is only relevant when using the makeflow backend, and is a workaround to stop pydpiper from trying to run makeflow itself. Should be fixed (i.e. no longer necessary) in a future version of pydpiper.\n\n4\n\nThe pipeline name - one of the required arguments. All output files and directories will have this name as its prefix, so it has to be unique within the directory from which the MBM command is being run.\n\n5\n\nThe subject matter - when set to mousebrain it will use a few defaults sensible for rodents (i.e. works for rats too) for some of the registration steps.\n\n6\n\nThe initial model - one of the key parameters for a successful registration, described in more detail below, see Section 1.\n\n7\n\nHow to run the initial 6 parameter (rigid body) alignment; also key for a successful registration, described in more detail in Section 2.\n\n8\n\nHere I’m over-riding the default parameters for the 12 parameter registration, with those parameters specified in the text file following the argument. A full description of how to craft those files will be the subject of a future post.\n\n9\n\nWe tell pydpiper to use MAGeT, our multi-atlas registration pipeline, to also segment the brains.\n\n10\n\nWe specify to use minctracc (rather than ANTS) for the MAGeT registrations. In a prior study we found that ANTS had better formed jacobians, and was thus preferable for the voxel based analyses and is thus the registration engine we use for most pydpiper registrations, but that minctracc created slightly more accurate segmentations and was a decent bit faster, hence we usually use it for the segmentation pipelines.\n\n11\n\nWe tell MAGeT to use a particular segmented atlas library, here the DSURQE atlas. Full description of the different atlases will be the subject of a future post.\n\n12\n\nNext are two more options for specifying the parameters, described in the text files following the arguments, to control how to run the MAGeT registrations.\n\n13\n\nThere is an optional step to align the final study-specific template to a common space shared across different registrations. Here we turn that off.\n\n14\n\nWe tell pydpiper to only use a single executor - this option only becomes relevant if we want pydpiper to execute commands itself, but since we opted to output a makeflow file instead and will execute the pipeline via makeflow, we switch this to a single executor.\n\n15\n\nAnd lastly we specify the MINC files that go into the pipeline run itself. Obviously a crucial argument.\nThere are many more options than this - to see them all, run MBM –help.\nThe next step in this pipeline would then be to use makeflow to run the commands; see an example for running a pipeline on the BMRC cluster."
  },
  {
    "objectID": "posts/MBM-command-illustrated/index.html#sec-init",
    "href": "posts/MBM-command-illustrated/index.html#sec-init",
    "title": "Annotated MBM command",
    "section": "1 Initial models",
    "text": "1 Initial models\nMost parameters to pydpiper pipelines can remain unchanged no matter whether the input is mouse or rat, in-vivo or ex-vivo. But initial models are different - they have to be matched to your data for the registration to work. I’ll cover how to create an initial model from scratch in a future post. In the meantime, here are the most used models at MICe and Oxford:\n\n\n\n\n\n\n\n\nName\nModality\nDescription\n\n\n\n\noxford-model-2023/oxford_t2w_mouse_60micron.mnc\nin-vivo mouse\nUse for Oxford MEMRI scans."
  },
  {
    "objectID": "posts/MBM-command-illustrated/index.html#sec-lsq6",
    "href": "posts/MBM-command-illustrated/index.html#sec-lsq6",
    "title": "Annotated MBM command",
    "section": "2 LSQ6 choices",
    "text": "2 LSQ6 choices\nIn my experience, the initial rigid body alignment stage is the one most likely to fail. I rarely ever change the lsq12 or nlin parameters, but when faced with new data I routinely modify the initial model and switch between lsq6 modes. There are three lsq6 modes built into pydpiper pipelines.\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--lsq6-simple\nAssumes that the initial model and the native files are in the same space. I use this for in-vivo data from the Oxford scanner, where there is a single coil and the positioning of the mice in the scanner is fairly consistent.\n\n\n--lsq6-centre-estimation\nAssumes that the initial model and the native files are oriented the same way, but estimates a centre-of-gravity based translation. This can work for the MICe in-vivo setup, where the orientation is well controlled but due to the multiple coils the start coordinates differ dependent on coils.\n\n\n--lsq6-large-rotations\nNeither the orientation nor the start coordinates are assumed to be the same. This option is essential for ex-vivo samples scanned in Toronto, especially for embryos where it is challenging to control the rotation of the sample precisely. There are two further options that control the search space: --lsq6-rotational-range and --lsq6-rotational-interval."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to this blog, which is mostly motivated by the poor state of documentation around various mouse brain imaging analysis tools, in particular the pydpiper pipelines and RMINC. So, rather than create a proper documentation website, here I will collect notes on different analyses and make them accessible for anyone else who might benefit. And who knows, the posts will likely expand to random stats topics and digressions in neuroscience."
  },
  {
    "objectID": "posts/welcome/index.html#how-to-contribute",
    "href": "posts/welcome/index.html#how-to-contribute",
    "title": "Welcome",
    "section": "How to contribute",
    "text": "How to contribute\nContributions are very much welcome! Next time you start a new analysis for which you take notes for yourself anyway, why not turn them into a blog post? The best way is to clone the repository for this blog (it’s in the quarto blog format), create a new post, and then make a pull request via github, and then I’ll add it to the blog. The repo for this repository is here."
  },
  {
    "objectID": "posts/sct-power-basic/index.html",
    "href": "posts/sct-power-basic/index.html",
    "title": "SCT power analyses",
    "section": "",
    "text": "This posts originates from a power analysis needed for a grant application. But it brings with it some potentially interesting methods for conducting power analyses in general alongside some discussion of power in non-orthogonal designs."
  },
  {
    "objectID": "posts/sct-power-basic/index.html#background",
    "href": "posts/sct-power-basic/index.html#background",
    "title": "SCT power analyses",
    "section": "Background",
    "text": "Background\nThe power analyses here concern the Sex Chromosome Trisomy (SCT) mouse model, which separates gonadal from chromosomal sex and varies sex chromosome dosage. To do that, the testes forming Sry gene is deleted from the Y chromosome and reinserted onto an autosome, thus allowing one to have XX mice with testes or XY mice with ovaries. In addition, a supernumerary sex chromosome is added, giving one XXY and XYY mice in addition to the XY and XX animals. In the end, there are thus 8 genotypes in the SCT model, as summarized in the table below:\n\n\n\nShorthand\nGonads\nX dose\nY dose\n\n\n\n\nXYM\nTestes\n1\n1\n\n\nXYF\nOvaries\n1\n1\n\n\nXXM\nTestes\n2\n0\n\n\nXXF\nOvaries\n2\n0\n\n\nXXYM\nTestes\n2\n1\n\n\nXXYF\nOvaries\n2\n1\n\n\nXYYM\nTestes\n1\n2\n\n\nXYYF\nOvaries\n1\n2\n\n\n\nThere are multiple ways to model this dataset, including testing for effects of 8 genotypes, the effect of aneuploidies, etc. But currently our thinking is that we can model the data as the effect of gonads, X chromosome dose, and Y chromosome dose."
  },
  {
    "objectID": "posts/sct-power-basic/index.html#setting-up-the-simulations",
    "href": "posts/sct-power-basic/index.html#setting-up-the-simulations",
    "title": "SCT power analyses",
    "section": "Setting up the simulations",
    "text": "Setting up the simulations\nLet’s set up a function that can simulate this data. First, load some libraries\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(splines)\n\n\nFor the sake of this post we’ll keep it simple, and not look for any interactions, and assume that the effects (at least as simulated) are uncorrelated. We’ll also look at what some large effect size (5 sd) simulations would look like to give us a sense of the data.\n\n\nShow the code\ngenerateSCTData &lt;- function(npergroup=10, \n                            gonadeffectmu=0,\n                            gonadeffectsd=0,\n                            xeffectmu=0,\n                            xeffectsd=0,\n                            yeffectmu=0,\n                            yeffectsd=0,\n                            epsilon=1) {\n  \n  # generate data from the normal distribution separately for each genotype. Here we just use\n  # epsilon for randomly distributed noise\n  groups &lt;- rbind(data.frame(group=\"XYM\", gonad=1,X=1,Y=1, volume=rnorm(npergroup, 0, epsilon)),\n                  data.frame(group=\"XYF\", gonad=0,X=1,Y=1, volume=rnorm(npergroup, 0, epsilon)),\n                  data.frame(group=\"XXM\", gonad=1,X=2,Y=0, volume=rnorm(npergroup, 0, epsilon)),\n                  data.frame(group=\"XXF\", gonad=0,X=2,Y=0, volume=rnorm(npergroup, 0, epsilon)),\n                  data.frame(group=\"XXYM\",gonad=1,X=2,Y=1, volume=rnorm(npergroup, 0, epsilon)),\n                  data.frame(group=\"XXYF\",gonad=0,X=2,Y=1, volume=rnorm(npergroup, 0, epsilon)),\n                  data.frame(group=\"XYYM\",gonad=1,X=1,Y=2, volume=rnorm(npergroup, 0, epsilon)),\n                  data.frame(group=\"XYYF\",gonad=0,X=1,Y=2, volume=rnorm(npergroup, 0, epsilon))) \n  \n  # and now we add the effects, again normally distributed (only matters if the sd terms are non-zero)\n  groups$volume &lt;- groups$volume + \n    rnorm(nrow(groups), gonadeffectmu * groups$gonad, gonadeffectsd) + \n    rnorm(nrow(groups), xeffectmu * groups$X, xeffectsd) + \n    rnorm(nrow(groups), yeffectmu * groups$Y, yeffectsd)\n    \n  # and return the dataset\n  return(groups %&gt;% mutate(\n    gonadeffectmu=gonadeffectmu,\n    xeffectmu=xeffectmu,\n    yeffectmu=yeffectmu\n  ))\n  \n} \n\n# now generate three datasets with large effects\nSCTg &lt;- generateSCTData(gonadeffectmu = 5) %&gt;% mutate(sim=\"Beta gonads = 5\")\nSCTx &lt;- generateSCTData(xeffectmu = 5) %&gt;% mutate(sim=\"Beta X = 5\")\nSCTy &lt;- generateSCTData(yeffectmu = 5) %&gt;% mutate(sim=\"Beta Y = 5\")\n\nrbind(SCTg, SCTx, SCTy) %&gt;% \n  ggplot() + \n  aes(x=group, y=volume) + \n  geom_boxplot() +\n  facet_grid(.~sim) + \n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust=1)) \n\n\n\n\n\nThis makes a few points that come out of the model. Gonads are fully balanced, but chromosome dosage is not. There are two possible levels of X (1 and 2), and three of Y (0, 1, and 2). And there is some correlation between them\n\n\nShow the code\ngenerateSCTData(npergroup = 1) %&gt;%\n  ggplot() + \n  aes(x=X, y=Y) + \n  geom_tile() + \n  xlab(\"X dose\") + \n  ylab(\"Y dose\") +\n  scale_x_continuous(breaks = c(1,2)) + \n  theme_minimal()\n\n\n\n\n\nThe design is not fully orthogonal - you cannot have an X dose of 1 and Y dose of 0, or and X dose of 2, and Y of 2."
  },
  {
    "objectID": "posts/sct-power-basic/index.html#sct-power",
    "href": "posts/sct-power-basic/index.html#sct-power",
    "title": "SCT power analyses",
    "section": "SCT power",
    "text": "SCT power\nWhat does the SCT model’s non-orthogonal design do to our power? Let’s see - we’ll run a set of simulations. It’ll be relatively straightforward:\n\nfor 500 simulations\n\nfor effect sizes ranging from 0 to 1.5 with steps of 0.1\n\nsimulate an SCT dataset of 10 subjects per genotype with one of the three terms (genotype, X dose, Y dose) varying by the effect size\nmodel it with y ~ gonads + Xdose + Ydose\nassess the number of the simulations where p &lt; 0.05\n\n\n\n\n\nShow the code\ngenerateSCTSimSeries &lt;- function(nsims=500,\n                                 npergroup=10,\n                                 gonadeffectmu=0,\n                                 xeffectmu=0,\n                                 yeffectmu=0,\n                                 epsilon=1){\n  # make sure that effects are all equal\n  if (length(gonadeffectmu) != length(xeffectmu) | \n      length(gonadeffectmu) != length(yeffectmu)) {\n    stop(\"Effects must be the same length\")\n  }\n  \n  neffects &lt;- length(gonadeffectmu)\n  \n  # and now we run the simulations\n  return(pmap(list(npergroup=rep(npergroup, neffects*nsims), \n                       gonadeffectmu=rep(gonadeffectmu, nsims), \n                       xeffectmu=rep(xeffectmu, nsims), \n                       yeffectmu=rep(yeffectmu, nsims),\n                       epsilon=rep(epsilon, neffects*nsims)),\n                  generateSCTData))\n\n}\n\nmodelSCTSimSeries &lt;- function(df) {\n  #df &lt;- df %&gt;% mutate(X = X-1)\n  tidy(lm(volume ~ gonad + X + Y, df)) %&gt;% mutate(gonadeffectmu=df$gonadeffectmu[1],\n                                                  xeffectmu=df$xeffectmu[1],\n                                                  yeffectmu=df$yeffectmu[1]) %&gt;%\n    pivot_wider(names_from = term, values_from = estimate:p.value)\n}\n\nmakePowerSeries &lt;- function() {\n  effectseries &lt;- seq(0, 1.5, 0.1)\n  neffects &lt;- length(effectseries)\n  npergroup=10\n  nsims=500\n  \n  Gseries &lt;- generateSCTSimSeries(nsims=nsims, npergroup = npergroup,\n                                     xeffect=rep(0, neffects), \n                                     yeffectmu = rep(0, neffects), \n                                     gonadeffectmu = effectseries) %&gt;%\n    map_dfr(modelSCTSimSeries) %&gt;% mutate(delta=gonadeffectmu, p=p.value_gonad)\n  Xseries &lt;- generateSCTSimSeries(nsims=nsims, npergroup = npergroup,\n                                     gonadeffect=rep(0, neffects), \n                                     yeffectmu = rep(0, neffects), \n                                     xeffectmu = effectseries) %&gt;%\n    map_dfr(modelSCTSimSeries) %&gt;% mutate(delta=xeffectmu, p=p.value_X)\n  Yseries &lt;- generateSCTSimSeries(nsims=nsims, npergroup = npergroup,\n                                     xeffect=rep(0, neffects), \n                                     gonadeffectmu = rep(0, neffects), \n                                     yeffectmu = effectseries) %&gt;%\n    map_dfr(modelSCTSimSeries) %&gt;% mutate(delta=yeffectmu, p=p.value_Y)\n  \n  allseries &lt;- rbind(Gseries, Xseries, Yseries)\n  return(allseries)\n}\n\nfullseries &lt;- makePowerSeries()\n\n\nLet’s see what this looks like:\n\n\nShow the code\nfullseries %&gt;% \n  group_by(yeffectmu, xeffectmu, gonadeffectmu) %&gt;% \n  summarise(power=mean(p&lt;0.05)) %&gt;% \n  pivot_longer(yeffectmu:gonadeffectmu) %&gt;% \n  ggplot() + \n  aes(x=value, y=power, colour=name) + \n  geom_point() + \n  geom_smooth(se=F, method=\"lm\", formula=y~ns(x,7)) + \n  xlab(bquote(delta ~ (effect ~ size))) + \n  scale_x_continuous(limits=c(0.05, 1.5)) + \n  scale_color_brewer(\"Model term\", labels=c(\"Gonad\", \"X dose\", \"Y dose\"), palette = \"Set1\") + \n  geom_hline(yintercept = 0.8) + \n  scale_y_continuous(breaks = seq(0,1,0.2)) + \n  theme_minimal()\n\n\n\n\n\nHere power - the proportion of simulations where p was &lt; 0.05 - is on the y axis, and the simulated effect size on the x axis. So, for example, to get a power of 0.08 you’d need to an effect size of about 0.6 for gonads and Y dose and 0.9 for X dose."
  },
  {
    "objectID": "posts/sct-power-basic/index.html#why-is-x-dose-less-powered-than-y-or-gonads",
    "href": "posts/sct-power-basic/index.html#why-is-x-dose-less-powered-than-y-or-gonads",
    "title": "SCT power analyses",
    "section": "Why is X dose less powered than Y or gonads?",
    "text": "Why is X dose less powered than Y or gonads?\nWhat’s up with the previous results? Why is X dose at a lower power than Y or gonads?\nLet’s start with gonads and test whether our simulations make sense. We simulated 10 mice per group, so given that exactly half the mice will be of each gonad type we can assess whether we’d get the same answer with a parametric power test:\n\n\nShow the code\npower.t.test(n=40, power=0.8)\n\n\n\n     Two-sample t test power calculation \n\n              n = 40\n          delta = 0.634299\n             sd = 1\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nAnd indeed, the answer is the same - at 40 mice per group and a power of 0.8 you’d recapture an effect size of 0.63.\nBut you also have 40 mice with one X chromosome and 40 mice with 2 X chromosomes, yet the power is lower than expected. The answer lies in the fact that the two are correlated:\n\n\nShow the code\ngenerateSCTData(npergroup = 1) %&gt;% select(gonad, X, Y) %&gt;% cor\n\n\n      gonad          X          Y\ngonad     1  0.0000000  0.0000000\nX         0  1.0000000 -0.7071068\nY         0 -0.7071068  1.0000000\n\n\nNow go and have a read of this paper from 1991. Estimated errors in linear models are both dependent on the covariance between terms as well as the range of possible values. So X and Y doses are correlated, but Y dose has a greater range, hence the greater power for Y dose than X dose in the SCT model."
  },
  {
    "objectID": "posts/sct-power-basic/index.html#the-dangers-of-model-misspecification",
    "href": "posts/sct-power-basic/index.html#the-dangers-of-model-misspecification",
    "title": "SCT power analyses",
    "section": "The dangers of model misspecification",
    "text": "The dangers of model misspecification\nSo if there is a problem with colinearity why don’t we run a separate model with just or X or Y dose? Let’s try that for a single simulation of an effect size of 2 for X\n\n\nShow the code\nset.seed(42)\nXsim &lt;- generateSCTData(xeffectmu = 2)\n\n\nAnd let’s look at the output of our linear model including all terms:\n\n\nShow the code\nsummary(lm(volume ~ gonad + X + Y, Xsim))\n\n\n\nCall:\nlm(formula = volume ~ gonad + X + Y, data = Xsim)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.71496 -0.56202  0.06254  0.65473  2.37194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.01185    0.71272  -0.017   0.9868    \ngonad        0.40373    0.23757   1.699   0.0933 .  \nX            1.80717    0.33598   5.379 7.99e-07 ***\nY            0.11938    0.23757   0.503   0.6168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 76 degrees of freedom\nMultiple R-squared:  0.4136,    Adjusted R-squared:  0.3905 \nF-statistic: 17.87 on 3 and 76 DF,  p-value: 7.13e-09\n\n\nLooks reasonably good in terms of both effect size and significance for both X and Y. But now let’s just model X.\n\n\nShow the code\nsummary(lm(volume ~ X, Xsim))\n\n\n\nCall:\nlm(formula = volume ~ X, data = Xsim)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8571 -0.6050  0.1795  0.6668  2.1104 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.4885     0.3784   1.291    0.201    \nX             1.6878     0.2393   7.053 6.25e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.07 on 78 degrees of freedom\nMultiple R-squared:  0.3894,    Adjusted R-squared:  0.3816 \nF-statistic: 49.74 on 1 and 78 DF,  p-value: 6.247e-10\n\n\nThe estimate is a bit lower, but still not too far off. But let’s model Y for data where only a change in X was simulated:\n\n\nShow the code\nsummary(lm(volume ~ Y, Xsim))\n\n\n\nCall:\nlm(formula = volume ~ Y, data = Xsim)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6766 -0.8249  0.2119  0.9249  2.5556 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.8044     0.2419   15.73  &lt; 2e-16 ***\nY            -0.7842     0.1975   -3.97 0.000159 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.249 on 78 degrees of freedom\nMultiple R-squared:  0.1681,    Adjusted R-squared:  0.1575 \nF-statistic: 15.76 on 1 and 78 DF,  p-value: 0.0001585\n\n\nA woefully incorrect estimate. In the presence of colinearity you need to covary for the other parameters to not get caught in a badly misspecified model."
  },
  {
    "objectID": "posts/developmental-data/index.html",
    "href": "posts/developmental-data/index.html",
    "title": "Developmental analyses, part 1",
    "section": "",
    "text": "Longitudinal data provides its own set of modelling challenges. These primarily concern how to model time, especially if there is a curvilinear relationship between time and variable(s) of interest. A related issue is making statistical models not only fit the data well but also produce interpretable output that helps address questions of changes in baseline alongside changes in trajectories.\nThis will be a series of posts, exploring:\n\nPlotting of longitudinal data\nFitting straight lines and interpreting slopes and intercepts\nFitting curves via splines\nComparisons between different fitted models for ease of interpretation\nSegmented models\nGeneral additive models\nRandom intercepts and random slopes\nBayesian linear models\nRMINC and data.tree methods to explore all these across the brain\n\nFor this first post we’ll look at points 1-4 with small bits of RMINC and data.tree creeping in.\nTo explore these questions we will use data being collected by Tiffany Chien, where she is looking at the effect of maternal auto-antibodies on brain development in mice. Each mouse was imaged at eight different timepoints, following a pattern set here and first developed here. We begin by loading the data as provided by Tiffany:\n\nlibrary(data.tree)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nload(\"brain_analysis_data_2023nov2.RData\")\n\nThis contains 5 items - the anatomy and labels volumes, the dataframe describing each scan (gf), a matrix of structure volumes (structvols), and the label definitions (hdefs).\nWe can then add the volumes to the hierarchical anatomy, since anatomical hierarchies are awesome.\n\nlibrary(RMINC)\n# the suppress warnings bit deals with a comparison in the data.tree\n# library that throws far too many warnings.\nhvols &lt;- suppressWarnings(addVolumesToHierarchy(hdefs, structvols))\n\nLet’s start with overall brain volumes\n\ngf$brainVols &lt;- hvols$volumes\n\nAnd plot them, initially by sex, since we have some decently strong expectations of the development of sex differences in the brain.\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# set some defaults for ggplots for the rest of the document\ntheme_set(theme_minimal())\nscale_colour_brewer_d &lt;- function(...) {\n  scale_colour_brewer(palette = \"Set1\", ...)\n}\n\noptions(ggplot2.discrete.colour=scale_colour_brewer_d)\n\nggplot(gf) + \n  aes(x=age, y=brainVols, colour=Sex) + \n  geom_point() + \n  geom_smooth() + \n  ylab(bquote(Volume ~ (mm^3))) + \n  ggtitle(\"Brain volume\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis is a fairly standard ggplot plot. geom_smooth does a lot of the heavy lifting here. By default it uses a local estimator (loess) to fit the curve; these are great for visualization but not so useful for analyses. It also shows a clear overall pattern of rapid growth in brain volume until around day 20, followed by slower growth.\nBrain volume does not appear particularly different by sex, which is what we have seen before. Since we are looking at sex let’s pick a classically dimorphic brain structure\n\nggplot(gf %&gt;% \n         mutate(MeA = FindNode(hvols, \"Medial amygdalar nucleus\")$volumes)) + \n  aes(x=age, y=MeA, colour=Sex) + \n  geom_point() + \n  geom_smooth() + \n  ylab(bquote(Volume ~ (mm^3))) + \n  ggtitle(\"Medial Amygdala\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nOK, we have sensible patterns, showing larger volumes of the medial amygdala in males, seemingly emerging over the first few days of life. Let’s improve the plot a little bit to let us see what happens to individual mice. First, let’s plot each mouse entirely separately.\n\nggplot(gf %&gt;% \n         mutate(MeA = FindNode(hvols, \"Medial amygdalar nucleus\")$volumes)) + \n  aes(x=age, y=MeA, colour=Sex) + \n  geom_point() + \n  geom_line() + \n  ylab(bquote(Volume ~ (mm^3))) + \n  ggtitle(\"Medial Amygdala\") + \n  facet_wrap(~subject_id)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\nNote that I’ve replaced the fitting (geom_smooth) with a simpler line (geom_line). A few features immediately stand out from this plot: quite a few mice only have a few data points, which is primarily due to data collection being ongoing, and there are too many mice for this plot to be terribly useful. Let’s instead plot separate lines on the same plot as before:\n\nggplot(gf %&gt;% \n         mutate(MeA = FindNode(hvols, \"Medial amygdalar nucleus\")$volumes)) + \n  aes(x=age, y=MeA, colour=Sex) + \n  geom_point() + \n  geom_smooth() + \n  geom_line(aes(group=subject_id), alpha=0.2) + \n  ylab(bquote(Volume ~ (mm^3))) + \n  ggtitle(\"Medial Amygdala\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNot always clear how useful such a plot is, but it sure is pretty. Note that the geom_line bit needed to have the group specified in order for ggplot to know which points to join up with lines.\nOn to statistics. For the sake of argument let’s go with the simplest model of volume against age by sex:\n\nsuppressMessages(library(lmerTest))\n# create a new variable for the medial amygdala for easier access\ngf &lt;- gf %&gt;% \n  mutate(MeA = FindNode(hvols, \"Medial amygdalar nucleus\")$volumes)\n\n# run the linear mixed effects model.\nsummary(lmer(MeA ~ age * Sex + (1|subject_id), gf))\n\nboundary (singular) fit: see help('isSingular')\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: MeA ~ age * Sex + (1 | subject_id)\n   Data: gf\n\nREML criterion at convergence: -362.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6383 -0.7678  0.0283  0.7920  3.6123 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n subject_id (Intercept) 0.000000 0.0000  \n Residual               0.009781 0.0989  \nNumber of obs: 224, groups:  subject_id, 37\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 5.125e-01  1.425e-02 2.200e+02  35.967   &lt;2e-16 ***\nage         7.674e-03  4.859e-04 2.200e+02  15.794   &lt;2e-16 ***\nSexM        5.855e-03  1.942e-02 2.200e+02   0.302   0.7633    \nage:SexM    1.717e-03  6.762e-04 2.200e+02   2.539   0.0118 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr) age    SexM  \nage      -0.739              \nSexM     -0.734  0.542       \nage:SexM  0.531 -0.719 -0.731\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nHere we use a linear mixed effects model. These models are the bread and butter of longitudinal analyses, as they allow you to account for the dependence created by having multiple datapoints from the same subject. There are multiple ways to specify these models, with the main decisions revolving around what fixed effects and what random effects to use. Fixed effects are what you would use in any old linear model; here we specify an age by sex interaction. Random effects, the bits inside the parentheses, determine how to treat the individual mice in this dataset. Here we use the simplest formula, (1 | id), which allows every mouse to have a separate intercept but not a separate slope. I.e. at baseline mouse 1 can have a larger or smaller volume than mouse 2, but from then on the effect of age will be the same for all mice. We could allow for separate slopes too, a topic which we will explore in a future post.\nWe also get warnings about singular fits - these are due to no variance in the random intercept (i.e. the separate intercept per subject) being identified. This can be indicative of a real problem and the need to change the model formula, but in simple cases like this can be ignored for now.\nOnto the results. Our model shows an effect of age and an age by sex interaction, but no effect of sex. Why is that? Two reasons:\n\nWe are fitting a straight line, where there clearly is a curve in the data\nWe are testing the effect of sex at age=0, which doesn’t make sense.\n\nLet’s look at them in turn. Plotting as a straight line:\n\nggplot(gf) +\n  aes(x=age, y=MeA, colour=Sex) + \n  geom_point() + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  ylab(bquote(Volume ~ (mm^3))) + \n  ggtitle(\"Medial Amygdala\")\n\n\n\n\nFirst, it’s evident that the model does not fit all that well, which is to be expected since we saw earlier that a straight line is not the right way to model age. Second, we can see the interaction - the lines are not parallel. Third, at age=0 the lines overlap.\nTackling the third point first, in a linear model with an interaction, each term of an interaction is interpreted at the zero point of the other term. Going back to the linear model output from above, it’s telling us that age=0 the females (the reference level of sex) have a volume of 5.125e-01 (the intercept), and males a volume of 5.125e-01 + 5.855e-03. If we change where the intercept falls we change these values. To whit, let’s test at an age of 65:\n\nsummary(lmer(MeA ~ age * Sex + (1|subject_id), gf %&gt;% mutate(age = age-65)))\n\nboundary (singular) fit: see help('isSingular')\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: MeA ~ age * Sex + (1 | subject_id)\n   Data: gf %&gt;% mutate(age = age - 65)\n\nREML criterion at convergence: -362.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6383 -0.7678  0.0283  0.7920  3.6123 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n subject_id (Intercept) 0.000000 0.0000  \n Residual               0.009781 0.0989  \nNumber of obs: 224, groups:  subject_id, 37\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 1.011e+00  2.315e-02 2.200e+02  43.693  &lt; 2e-16 ***\nage         7.674e-03  4.859e-04 2.200e+02  15.794  &lt; 2e-16 ***\nSexM        1.175e-01  3.256e-02 2.200e+02   3.607 0.000383 ***\nage:SexM    1.717e-03  6.762e-04 2.200e+02   2.539 0.011796 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr) age    SexM  \nage       0.910              \nSexM     -0.711 -0.647       \nage:SexM -0.654 -0.719  0.914\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nI changed age by subtracting 65 from it, so now the intercept (age=0) is really age=65. Now the SexM term has become quite significant; look back at the plot above and see how the two lines diverge significantly at that age. At age 65 the females have a volume 1.01, and males 1.01 + 0.11.\n\nFitting curves in linear models\nNow back to the fact that the data should not be fit with a straight line since there is a clearly detectable curve in the relation between volume and age. Let’s use a third order spline:\n\nlibrary(splines)\nggplot(gf) +\n  aes(x=age, y=MeA, colour=Sex) + \n  geom_point() + \n  geom_smooth(method=\"lm\", formula=y~ns(x,3) ) + \n  ylab(bquote(Volume ~ (mm^3))) + \n  ggtitle(\"Medial Amygdala\")\n\n\n\n\nHere I changed the bit inside geom_smooth. First, using the method=\"lm\" syntax I told it to use a linear model rather than a local estimator, and then I gave it the formula. The ns(age, 3) syntax tells it to use a natural spline with 3 degrees of freedom.\nThat fits pretty well. Let’s look at the model outputs\n\nsummary(lmer(MeA ~ ns(age,3) * Sex + (1|subject_id), gf))\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: MeA ~ ns(age, 3) * Sex + (1 | subject_id)\n   Data: gf\n\nREML criterion at convergence: -684.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4337 -0.5530  0.1326  0.5837  4.8808 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n subject_id (Intercept) 0.000588 0.02425 \n Residual               0.001906 0.04366 \nNumber of obs: 224, groups:  subject_id, 37\n\nFixed effects:\n                  Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)      4.357e-01  1.200e-02 1.420e+02  36.326  &lt; 2e-16 ***\nns(age, 3)1      4.611e-01  2.098e-02 1.873e+02  21.977  &lt; 2e-16 ***\nns(age, 3)2      6.464e-01  2.463e-02 1.898e+02  26.238  &lt; 2e-16 ***\nns(age, 3)3      3.878e-01  1.304e-02 1.856e+02  29.730  &lt; 2e-16 ***\nSexM             3.758e-03  1.596e-02 1.356e+02   0.235 0.814249    \nns(age, 3)1:SexM 5.144e-02  2.931e-02 1.895e+02   1.755 0.080902 .  \nns(age, 3)2:SexM 1.110e-01  3.287e-02 1.893e+02   3.378 0.000888 ***\nns(age, 3)3:SexM 7.340e-02  1.833e-02 1.860e+02   4.005 8.94e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) ns(,3)1 ns(,3)2 ns(,3)3 SexM   n(,3)1: n(,3)2:\nns(age, 3)1 -0.002                                               \nns(age, 3)2 -0.782 -0.029                                        \nns(age, 3)3 -0.193  0.167   0.314                                \nSexM        -0.751  0.002   0.587   0.145                        \nns(g,3)1:SM  0.002 -0.716   0.021  -0.120  -0.002                \nns(g,3)2:SM  0.586  0.022  -0.749  -0.235  -0.766 -0.030         \nns(g,3)3:SM  0.137 -0.119  -0.223  -0.712  -0.178  0.151   0.311 \n\n\nNow we have a much harder to interpret set of age terms (and their interactions). Each of the ns(age, 3)1 or ns(age, 3)2 or ns(age, 3)3 tells us about the parameters of the different spline terms. While these can obviously be used to reconstruct the fit, in practice they are essentially uninterpretable for figuring out where and how the age trajectories deviate by sex.\nBut looking at just the sex terms, it’s again insignificant at age=0. Let’s see at age 65.\n\nsummary(lmer(MeA ~ ns(age,3) * Sex + (1|subject_id), gf %&gt;% mutate(age=age-65)))\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: MeA ~ ns(age, 3) * Sex + (1 | subject_id)\n   Data: gf %&gt;% mutate(age = age - 65)\n\nREML criterion at convergence: -684.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4337 -0.5530  0.1326  0.5837  4.8808 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n subject_id (Intercept) 0.000588 0.02425 \n Residual               0.001906 0.04366 \nNumber of obs: 224, groups:  subject_id, 37\n\nFixed effects:\n                  Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)      4.357e-01  1.200e-02 1.420e+02  36.326  &lt; 2e-16 ***\nns(age, 3)1      4.611e-01  2.098e-02 1.873e+02  21.977  &lt; 2e-16 ***\nns(age, 3)2      6.464e-01  2.463e-02 1.898e+02  26.238  &lt; 2e-16 ***\nns(age, 3)3      3.878e-01  1.304e-02 1.856e+02  29.730  &lt; 2e-16 ***\nSexM             3.758e-03  1.596e-02 1.356e+02   0.235 0.814249    \nns(age, 3)1:SexM 5.144e-02  2.931e-02 1.895e+02   1.755 0.080902 .  \nns(age, 3)2:SexM 1.110e-01  3.287e-02 1.893e+02   3.378 0.000888 ***\nns(age, 3)3:SexM 7.340e-02  1.833e-02 1.860e+02   4.005 8.94e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) ns(,3)1 ns(,3)2 ns(,3)3 SexM   n(,3)1: n(,3)2:\nns(age, 3)1 -0.002                                               \nns(age, 3)2 -0.782 -0.029                                        \nns(age, 3)3 -0.193  0.167   0.314                                \nSexM        -0.751  0.002   0.587   0.145                        \nns(g,3)1:SM  0.002 -0.716   0.021  -0.120  -0.002                \nns(g,3)2:SM  0.586  0.022  -0.749  -0.235  -0.766 -0.030         \nns(g,3)3:SM  0.137 -0.119  -0.223  -0.712  -0.178  0.151   0.311 \n\n\nThe model terms haven’t changed. So with splines we’ve lost the ability to interpret our data quite the way we want by moving the intercept; other spline parameterizations than natural splines can do better, and polynomials can do it very well, but ultimately we want a more general solution. The emmeans package to the rescue for estimated marginal means.\n\nlibrary(emmeans)\nl &lt;- lmer(MeA ~ ns(age,3) * Sex + (1|subject_id), gf)\ne &lt;- emmeans(l, ~ Sex | age, at = list(age=c(7, 21, 65)))\n\nFirst, we fitted a linear mixed effects model as before, and then we’ve computed the marginal means using the emmeans function from the package of the same name. The first argument is the model, then it’s the formula; here we tell it that we want to evaluate Sex across age. Next we tell the emmeans function to evaluate its output at 3 different ages - 7, 21, and 65. We can look at the output:\n\ne\n\nage =  7:\n Sex emmean      SE    df lower.CL upper.CL\n F    0.525 0.00863  56.6    0.508    0.543\n M    0.547 0.00807  56.0    0.531    0.563\n\nage = 21:\n Sex emmean      SE    df lower.CL upper.CL\n F    0.766 0.00908  65.2    0.748    0.784\n M    0.820 0.00866  71.0    0.802    0.837\n\nage = 65:\n Sex emmean      SE    df lower.CL upper.CL\n F    0.911 0.01320 172.9    0.885    0.937\n M    1.006 0.01310 184.7    0.980    1.032\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\nThis is already quite useful, as we can get the estimated means at those three ages for each sex, alongside their 95% confidence intervals. But we can go further and compute the differences at those ages explicitly:\n\npairs(e)\n\nage =  7:\n contrast estimate     SE    df t.ratio p.value\n F - M     -0.0216 0.0118  56.4  -1.832  0.0722\n\nage = 21:\n contrast estimate     SE    df t.ratio p.value\n F - M     -0.0538 0.0125  67.9  -4.289  0.0001\n\nage = 65:\n contrast estimate     SE    df t.ratio p.value\n F - M     -0.0951 0.0186 178.8  -5.115  &lt;.0001\n\nDegrees-of-freedom method: kenward-roger \n\n\nWe can tell from this output that sex differences are borderline at age 7, and become quite strikingly significant later. Note that the contrast is the reverse of what we would expect from our linear model; we can reverse that in turn:\n\npairs(e, reverse = T)\n\nage =  7:\n contrast estimate     SE    df t.ratio p.value\n M - F      0.0216 0.0118  56.4   1.832  0.0722\n\nage = 21:\n contrast estimate     SE    df t.ratio p.value\n M - F      0.0538 0.0125  67.9   4.289  0.0001\n\nage = 65:\n contrast estimate     SE    df t.ratio p.value\n M - F      0.0951 0.0186 178.8   5.115  &lt;.0001\n\nDegrees-of-freedom method: kenward-roger \n\n\nAs a way of comparison, let’s get back to overall brain volume, which at least visually appeared to not differ much by sex:\n\nl &lt;- lmer(brainVols ~ ns(age,3) * Sex + (1|subject_id), gf)\n(e &lt;- emmeans(l, ~ Sex | age, at = list(age=c(7, 21, 65))))\n\nage =  7:\n Sex emmean   SE    df lower.CL upper.CL\n F      236 2.65  46.4      231      242\n M      242 2.48  47.1      237      247\n\nage = 21:\n Sex emmean   SE    df lower.CL upper.CL\n F      380 2.75  51.4      374      385\n M      385 2.59  55.0      380      390\n\nage = 65:\n Sex emmean   SE    df lower.CL upper.CL\n F      428 3.58 122.2      421      435\n M      431 3.51 138.1      424      438\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\npairs(e, reverse = T)\n\nage =  7:\n contrast estimate   SE    df t.ratio p.value\n M - F        5.64 3.63  46.7   1.554  0.1269\n\nage = 21:\n contrast estimate   SE    df t.ratio p.value\n M - F        5.53 3.78  53.1   1.462  0.1495\n\nage = 65:\n contrast estimate   SE    df t.ratio p.value\n M - F        2.92 5.01 129.8   0.583  0.5609\n\nDegrees-of-freedom method: kenward-roger \n\n\nAnd indeed the sex difference in brain volume is relatively uninteresting at those three ages.\nTwo more digressions with emmeans. First is computing effect sizes, measured as Cohen’s d:\n\nl &lt;- lmer(MeA ~ ns(age,3) * Sex + (1|subject_id), gf)\ne &lt;- emmeans(l, ~ Sex | age, at = list(age=c(7, 21, 65)))\neff_size(e, sigma=sigma(l), edf=30)\n\nage =  7:\n contrast effect.size    SE    df lower.CL upper.CL\n F - M         -0.496 0.278  56.0    -1.05   0.0612\n\nage = 21:\n contrast effect.size    SE    df lower.CL upper.CL\n F - M         -1.233 0.329  65.2    -1.89  -0.5766\n\nage = 65:\n contrast effect.size    SE    df lower.CL upper.CL\n F - M         -2.179 0.510 172.9    -3.19  -1.1712\n\nsigma used for effect sizes: 0.04366 \nDegrees-of-freedom method: inherited from kenward-roger when re-gridding \nConfidence level used: 0.95 \n\n\nA few things to note:\n\nThe effect size function needs the model, the sigma from the linear mixed effects model, and the degrees of freedom of sigma, edf. This latter edf is very hard to define, which comes back to acknowledging that the confidence intervals around these effect sizes are going to be somewhat ill defined. Still useful, but keep that in mind.\nThe direction of the effect sizes has reversed again, and there appears to be no reverse=TRUE equivalent to what can be had in pairs.\n\nOne more digression on emmeans. Since the effects are conditional on the model, that means we can evaluate them at any age, not just those where we had scans. This in turn can be useful for visualizing the temporal evolution of your contrast differences. To wit:\n\nlibrary(broom)\ne &lt;- emmeans(l, ~ Sex | age, at = list(age=seq(3, 65, by=5)))\nef &lt;- eff_size(e, sigma=sigma(l), edf=30)\ntidy(ef, conf.int = T) %&gt;% \n  ggplot() + \n  aes(x=age, y=estimate*-1, ymin=conf.low*-1, ymax=conf.high*-1) + \n  geom_ribbon(alpha=0.3) + \n  geom_line() + \n  geom_hline(yintercept = 0, linetype=2) + \n  ylab(\"Effect size (male - female)\") + \n  ggtitle(\"Medial Amygdala\", subtitle = \"Effect size of sex difference\")\n\n\n\n\nThis shows the effect sizes of sex in the medial amygdala steadily increasing with age. (And note the multiplication by -1 to get the direction to be what we expected. Also note that we are using the tidy function from the broom package to get clean output from emmeans).\n\n\nUsing model comparisons\nThe downside of the emmeans approach is that one has to, in effect, test multiple different ages, which only makes sense when there are precise hypotheses. An alternate approach is to do a set of model comparisons to get at what we would normally think of as main effects. I.e. how much better does the model fit when we include sex? And do we need an interaction? Let’s test that:\n\nsimpleAgeModel &lt;- lmer(MeA ~ ns(age, 3) + (1|subject_id), gf, REML = F)\nAgeAndSexModel &lt;- lmer(MeA ~ ns(age, 3) + Sex + (1|subject_id), gf, REML = F)\nAgeAndSexInteractionModel  &lt;- lmer(MeA ~ ns(age, 3) * Sex + (1|subject_id), gf, REML = F)\n\nanova(simpleAgeModel, AgeAndSexModel, AgeAndSexInteractionModel)\n\nData: gf\nModels:\nsimpleAgeModel: MeA ~ ns(age, 3) + (1 | subject_id)\nAgeAndSexModel: MeA ~ ns(age, 3) + Sex + (1 | subject_id)\nAgeAndSexInteractionModel: MeA ~ ns(age, 3) * Sex + (1 | subject_id)\n                          npar     AIC     BIC logLik deviance  Chisq Df\nsimpleAgeModel               6 -690.20 -669.73 351.10  -702.20          \nAgeAndSexModel               7 -701.50 -677.62 357.75  -715.50 13.305  1\nAgeAndSexInteractionModel   10 -717.96 -683.84 368.98  -737.96 22.453  3\n                          Pr(&gt;Chisq)    \nsimpleAgeModel                          \nAgeAndSexModel             0.0002647 ***\nAgeAndSexInteractionModel  5.251e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere we set up three different statistical models. The first, simpleAgeModel, has the third order spline for age but no mention of sex. This effectively becomes our null hypothesis - of course the brain will change with development, but it will not do so in any sex specific way. The second model, AgeAndSexModel, includes an additive effect for sex; this would allow for a separate offset by sex, but would keep the slopes between the medial amygdala and age the same for both sexes. The last model AgeAndSexInteractionModel, then also allows the age-relation to vary by sex. Once we’ve computed these three models we run a log-likelihood test between them using the anova function, which produces a chi-squared statistic and its associated p-value.\n(Note that we fit all the models with REML=FALSE. This tells lmer to use maximum likelihood, rather than restricted maximum likelihoods as by default. This switch to maximum likelihood estimation is required for the log-likelihood test to be valid when fixed effects are changing between models)\nThe results show, in looking at the p values, that the AgeAndSex model is better than the simple age model, and that the one with interactions is the best one of all. So for the medial amygdala sex matters and how it matters changes with age.\nLet’s retest for overall brain volume where again we should not expect much change:\n\nsimpleAgeModel &lt;- lmer(brainVols ~ ns(age, 3) + (1|subject_id), gf, REML = F)\nAgeAndSexModel &lt;- lmer(brainVols ~ ns(age, 3) + Sex + (1|subject_id), gf, REML = F)\nAgeAndSexInteractionModel  &lt;- lmer(brainVols ~ ns(age, 3) * Sex + (1|subject_id), gf, REML = F)\n\nanova(simpleAgeModel, AgeAndSexModel, AgeAndSexInteractionModel)\n\nData: gf\nModels:\nsimpleAgeModel: brainVols ~ ns(age, 3) + (1 | subject_id)\nAgeAndSexModel: brainVols ~ ns(age, 3) + Sex + (1 | subject_id)\nAgeAndSexInteractionModel: brainVols ~ ns(age, 3) * Sex + (1 | subject_id)\n                          npar    AIC    BIC  logLik deviance  Chisq Df\nsimpleAgeModel               6 1753.1 1773.6 -870.57   1741.1          \nAgeAndSexModel               7 1752.6 1776.5 -869.29   1738.6 2.5620  1\nAgeAndSexInteractionModel   10 1758.2 1792.3 -869.08   1738.2 0.4193  3\n                          Pr(&gt;Chisq)\nsimpleAgeModel                      \nAgeAndSexModel                0.1095\nAgeAndSexInteractionModel     0.9362\n\n\nThis once again says that sex does not play much of a role in shaping the overall size of the brain in mice.\nThat’s enough for now. Computing all these things across brain structures to follow, alongside a discussion of general additive models and segmented models and how to deal with 3 way interactions, and finally some Bayesian fun likely at the end."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "digressions on (mouse) brain imaging",
    "section": "",
    "text": "Developmental analyses, part 1\n\n\n\n\n\n\n\nR\n\n\nlongitudinal\n\n\n\n\nHow to deal with longitudinal brain development data\n\n\n\n\n\n\nNov 26, 2023\n\n\nJason Lerch\n\n\n\n\n\n\n  \n\n\n\n\nCopying important pydpiper outputs\n\n\n\n\n\n\n\npydpiper\n\n\n\n\nSome rsync commands to only copy important bits from a pydpiper pipeline\n\n\n\n\n\n\nOct 19, 2023\n\n\nJason Lerch\n\n\n\n\n\n\n  \n\n\n\n\nBetter MRIcrotome brain outlines\n\n\n\n\n\n\n\nR\n\n\nMRIcrotome\n\n\nRMINC\n\n\n\n\nHow to make better brain outlines for MRIcrotome\n\n\n\n\n\n\nSep 16, 2023\n\n\nJason Lerch\n\n\n\n\n\n\n  \n\n\n\n\nSCT power analyses\n\n\n\n\n\n\n\nR\n\n\n\n\nUnderstanding power for the SCT mouse model with some digressions into power in non-orthogonal designs\n\n\n\n\n\n\nSep 6, 2023\n\n\nJason Lerch and Kamila Szulc-Lerch\n\n\n\n\n\n\n  \n\n\n\n\nAnnotated MBM command\n\n\n\n\n\n\n\npydpiper\n\n\n\n\nAn annotated version of a simple MBM.py command to give a sense of all those pesky options\n\n\n\n\n\n\nSep 2, 2023\n\n\nJason Lerch\n\n\n\n\n\n\n  \n\n\n\n\nMINC tools on BMRC cluster\n\n\n\n\n\n\n\nOxford\n\n\n\n\nHow to use the MINC tools, including RMINC in RStudio, on Oxford’s BMRC cluster\n\n\n\n\n\n\nAug 29, 2023\n\n\nJason Lerch\n\n\n\n\n\n\n  \n\n\n\n\nWelcome\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nJason Lerch\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I figured I wrote enough notes for myself or for one specific purpose, mostly on the analysis of mouse brain imaging data, that I might as well put in a bit more effort and make the accessible. Contributions are very much welcome."
  }
]