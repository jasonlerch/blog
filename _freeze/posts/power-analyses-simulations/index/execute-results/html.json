{
  "hash": "7a2597386269657e7f6abae0c3479a13",
  "result": {
    "markdown": "---\ntitle: \"Power analyses via simulations\"\ndescription: \"How to create power analyses (in particular for longitudinal designs) via simulations\"\nauthor: \"Jason Lerch\"\ndate: \"2024-03-28\"\ndraft: true\ncategories: [R]\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n\n## Overview\n\nBackground rationale for power analyses via simulations goes here\n\n## Simple case\n\nTwo group power analysis. Let's assume that our outcome has a standard deviation of 1, that we have 10 participants per group, and we want to know what our power would be to detect a change of 0.5. This can be done parametrically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n=10, delta=0.5, sd=1, sig.level=0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     Two-sample t test power calculation \n\n              n = 10\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.1838375\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n:::\n:::\n\n\nThis tells us we have a power of 0.18. In other words, under these setting if we were to run a 100 experiments, 180 of them would find significance.\n\nLet's redo this as a simulation. Let's create a function for this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nsimSimpleGroupDiff <- function(n=10, delta=0.5, sd=1) {\n  tibble(\n    y = c(\n      rnorm(n=n, mean=0, sd=sd),\n      rnorm(n=n, mean=delta, sd=sd)),\n    group = rep(c(\"Control\", \"Intervention\"), each=10)\n  )\n}\n```\n:::\n\n\nThis function relies on rnorm to generate random normally distributed data.\n\nHere's an example of what the output would look like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimSimpleGroupDiff()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 2\n         y group       \n     <dbl> <chr>       \n 1 -0.215  Control     \n 2 -0.378  Control     \n 3  0.754  Control     \n 4 -0.162  Control     \n 5  0.718  Control     \n 6 -1.44   Control     \n 7  0.377  Control     \n 8  1.60   Control     \n 9  1.45   Control     \n10 -1.00   Control     \n11  0.645  Intervention\n12  1.28   Intervention\n13  0.356  Intervention\n14 -0.947  Intervention\n15  0.252  Intervention\n16  1.22   Intervention\n17 -0.0995 Intervention\n18 -0.637  Intervention\n19  1.35   Intervention\n20  0.224  Intervention\n```\n:::\n:::\n\n\nLet's write a function to run our statistical test. We'll stick to the linear model for simplicity's sake:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nmodelSimpleGroupDiff <- function(df) {\n  lm(y ~ group, df) %>% tidy() %>% filter(term == \"groupIntervention\")\n}\n```\n:::\n\n\nWe use a standard linear model, then tidy the output using the tidy function from the broom library, then discard the intercept and just keep the effects we are after.\n\nAnd see what it looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- simSimpleGroupDiff()\nmodelSimpleGroupDiff(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term              estimate std.error statistic p.value\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\n1 groupIntervention     1.18     0.359      3.29 0.00410\n```\n:::\n:::\n\n\nAnd now we can do this 1000 times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42) # for consistency across runs\nsimpleGroupSim <- map_dfr(1:1000, ~ modelSimpleGroupDiff(simSimpleGroupDiff()))\n```\n:::\n\n\nAnd now the test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(simpleGroupSim$p.value < 0.05) / 1000\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.178\n```\n:::\n:::\n\n\nSo our estimated power from this simulation is 0.178, which is decently close to the parametric result of 0.184.\n\nOf course the simulations can be expanded by adding an outer loop for different effects or group sizes. Here's an example for effects. First, a function to simplify the process:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimOneSimpleEffect <- function(delta=0.5, nSims=1000) {\n  simpleGroupSim <- map_dfr(1:nSims, \n                            ~ modelSimpleGroupDiff(simSimpleGroupDiff(delta = delta)))\n  sum(simpleGroupSim$p.value < 0.05) / nSims\n}\n```\n:::\n\n\nAnd now we can run this for a set of effects:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\neffectRange <- seq(0, 2, by=0.5)\nsimmedEffects <- map_dbl(effectRange, ~ simOneSimpleEffect(delta=.x))\ntibble(delta=effectRange, power=simmedEffects) %>%\n  ggplot() + aes(x=delta, y=power) + geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThis is essence captures the approach of using simulations to determine power.\n\n## A 2 timepoint longitudinal study\n\nLet's make the case a bit more complicated. 2 timepoints, consisting of a baseline, some intervention, followed by a post-intervention scan. The two groups are assumed to the be the same at baseline. First a function to simulate the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim2Timepoints <- function(n=10, \n                           baselineSd=1, \n                           interventionMean=0.5, \n                           interventionSd = 0.1) {\n    tibble(\n      baseline = rnorm(n*2, mean=0, sd=baselineSd),\n      group = rep(c(\"Control\", \"Intervention\"), each=10),\n      subject = paste(\"subject\", 1:20, sep=\"-\")) %>%\n    rowwise() %>%\n    mutate(followup = ifelse(group == \"Control\", \n                             baseline + rnorm(1, 0, interventionSd),\n                             baseline + rnorm(1, interventionMean, interventionSd)))\n}\n```\n:::\n\n\nLet's see what this looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(df <- sim2Timepoints())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 4\n# Rowwise: \n   baseline group        subject    followup\n      <dbl> <chr>        <chr>         <dbl>\n 1  -0.483  Control      subject-1   -0.332 \n 2   0.0735 Control      subject-2    0.0999\n 3   0.0244 Control      subject-3    0.149 \n 4   1.07   Control      subject-4    1.10  \n 5  -0.533  Control      subject-5   -0.530 \n 6   0.573  Control      subject-6    0.499 \n 7  -1.09   Control      subject-7   -1.20  \n 8   0.281  Control      subject-8    0.260 \n 9  -2.57   Control      subject-9   -2.79  \n10  -2.55   Control      subject-10  -2.56  \n11  -0.990  Intervention subject-11  -0.582 \n12   0.557  Intervention subject-12   1.01  \n13   0.448  Intervention subject-13   0.773 \n14  -0.443  Intervention subject-14   0.168 \n15   0.0872 Intervention subject-15   0.644 \n16  -1.15   Intervention subject-16  -0.682 \n17  -0.500  Intervention subject-17   0.128 \n18   0.342  Intervention subject-18   0.848 \n19  -0.888  Intervention subject-19  -0.474 \n20  -0.908  Intervention subject-20  -0.518 \n```\n:::\n:::\n\n\nNow we can create a set of different modelling functions to run against this data.\n\nFirst, the simplest - just test the followup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfollowupModel <- function(df) {\n  lm(followup ~ group, df) %>% tidy() %>% filter(term == \"groupIntervention\")\n}\nfollowupModel(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term              estimate std.error statistic p.value\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\n1 groupIntervention    0.661     0.457      1.45   0.165\n```\n:::\n:::\n\n\nNext, what should be the most powerful model - test the followup, covarying for baseline\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfollowupModelCovaryBaseline <- function(df) {\n  lm(followup ~ baseline + group, df) %>% tidy() %>% filter(term == \"groupIntervention\")\n}\nfollowupModelCovaryBaseline(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term              estimate std.error statistic       p.value\n  <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n1 groupIntervention    0.480    0.0456      10.5 0.00000000730\n```\n:::\n:::\n\n\nAnd finally an interaction model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmerTest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lme4\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Matrix'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lmerTest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:lme4':\n\n    lmer\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    step\n```\n:::\n\n```{.r .cell-code}\nlibrary(broom.mixed)\ninteractionModel <- function(df) {\n  # have to cast the dataframe\n  df %>% pivot_longer(c(baseline, followup), \n                      names_to=\"timepoint\", values_to=\"y\") %>%\n    lmer(y ~ timepoint * group + (1|subject), .) %>% tidy() %>%\n    filter(term == \"timepointfollowup:groupIntervention\")\n}\nfollowupModelCovaryBaseline(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term              estimate std.error statistic       p.value\n  <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n1 groupIntervention    0.480    0.0456      10.5 0.00000000730\n```\n:::\n:::\n\n\nClearly the two models that take baseline into account do much better. Let's compare them explicitly, but using a smaller effect. First, create the simulations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim2TimepointEffects <- map(1:500, ~ sim2Timepoints(interventionMean = 0.2))\n```\n:::\n\n\nNow run the separate models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfollowupModels <- map_dfr(sim2TimepointEffects, followupModel)\nfollowupModelCovaryBaselines <- map_dfr(sim2TimepointEffects,\n                                        followupModelCovaryBaseline)\ninteractionModels <- map_dfr(sim2TimepointEffects, interactionModel)\n```\n:::\n\n\nAnd compare\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(followupModels$p.value < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 26\n```\n:::\n\n```{.r .cell-code}\nsum(followupModelCovaryBaselines$p.value < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 489\n```\n:::\n\n```{.r .cell-code}\nsum(interactionModels$p.value < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 494\n```\n:::\n:::\n\n\nSo here it looks like we are almost fully powered for both the models accounting for baseline.\n\nThis could now be extended to test ranges of effects, baseline variation, group sizes, etc.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}